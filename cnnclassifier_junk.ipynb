{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwWbySmqy3u06mzdKn8zjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujathasivaraman/mlai/blob/main/cnnclassifier_junk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF_ykiWxEQSH"
      },
      "outputs": [],
      "source": [
        "#@title TEST for CNN :  Run this to load images and imports! {\"display-mode\":\"form\", \"form-width\":\"25%\"}\n",
        "#@title {\"display-mode\":\"form\", \"form-width\":\"25%\"}\n",
        "#@markdown ##**BEFORE RUNNING ANY CODE, please change your Hardware Accelerator to GPU to train faster!**</h2>\n",
        "#@markdown 1. Click on the **Runtime** menu at the top of the screen.\n",
        "#@markdown 2. Click **Change Runtime Type**.\n",
        "#@markdown 3. Choose **T4 GPU** under **Hardware Accelerator**.\n",
        "\n",
        "#@markdown Once you've done that, run this code cell to check you're correctly connected!\n",
        "\n",
        "import tensorflow as tf\n",
        "from IPython.display import Markdown\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "  display(Markdown(\"###‚úÖ GPU connected!\"))\n",
        "else:\n",
        "  display(Markdown(\"\"\"\n",
        "###‚ùå No GPU found!\n",
        "If you're running into GPU limits when you try to switch, here are some suggestions:\n",
        "  - Wait 12-24 hours for the limits to reset.\n",
        "  - Share your copy of the notebook with another Google account that hasn't met the limit, and work through the notebook with that account.\n",
        "  - Look into a paid subscription or paying for compute units as you go.\n",
        "  \"\"\"))\n",
        "  #@title **üèó Setup Cell** {\"display-mode\":\"form\", \"form-width\":\"25%\"}\n",
        "#@markdown **Run this to import libraries and download data!**\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# IMPORTS\n",
        "#-------------------------------------------------------------------------------\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, InputLayer\n",
        "from tensorflow.keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Inspirit's util file and discussion exercise answer handler\n",
        "!wget -q \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Modules/inspiritai_util.py\"\n",
        "from inspiritai_util import handle_discussion_response\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# HELPER FUNCTIONS\n",
        "#-------------------------------------------------------------------------------\n",
        "def categorical_to_numpy(labels_in):\n",
        "  labels = []\n",
        "  for label in labels_in:\n",
        "    if label == 'dog':\n",
        "      labels.append(np.array([1, 0]))\n",
        "    else:\n",
        "      labels.append(np.array([0, 1]))\n",
        "  return np.array(labels)\n",
        "\n",
        "def one_hot_encoding(input):\n",
        "  output = np.zeros((input.size, input.max()+1))\n",
        "  output[np.arange(input.size), input] = 1\n",
        "\n",
        "  return output\n",
        "\n",
        "def load_data():\n",
        "  # Run this cell to download our data into a file called 'cifar_data'\n",
        "  !wget -q --show-progress -O cifar_data https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%204%20_%205%20-%20Neural%20Networks%20_%20CNN/dogs_v_roads\n",
        "\n",
        "  # now load the data from our cloud computer\n",
        "  import pickle\n",
        "  with open(\"cifar_data\", \"rb\") as f:\n",
        "      data_dict = pickle.load(f)\n",
        "\n",
        "  data   = data_dict['data']\n",
        "  labels = data_dict['labels']\n",
        "\n",
        "  return data, labels\n",
        "\n",
        "def plot_one_image(data, labels, img_idx):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  img   = data[img_idx, :].reshape([32,32,3]).copy()\n",
        "  label = labels[img_idx]\n",
        "\n",
        "  fig, ax = plt.subplots(1,1)\n",
        "  img = ax.imshow(img.astype('uint8'), extent=[-1,1,-1,1])\n",
        "\n",
        "  x_label_list = [0, 8, 16, 24, 32]\n",
        "  y_label_list = [0, 8, 16, 24, 32]\n",
        "\n",
        "  ax.set_xticks([-1, -0.5, 0, 0.5, 1])\n",
        "  ax.set_yticks([-1, -0.5, 0, 0.5, 1])\n",
        "\n",
        "  ax.set_xticklabels(x_label_list)\n",
        "  ax.set_yticklabels(y_label_list)\n",
        "\n",
        "  ax.set_title(f'Image: {img_idx} | Label: {label}')\n",
        "\n",
        "  display(fig)\n",
        "  plt.close(fig)\n",
        "\n",
        "def plot_acc(history, ax=None, xlabel='Epoch #'):\n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_accuracy'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.5, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n",
        "    ax.legend(loc = 'best')\n",
        "    ax.set_ylim([0.4, 1.005])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def logits_to_one_hot_encoding(input):\n",
        "    \"\"\"\n",
        "    Converts softmax output (logits) to a one-hot encoded format.\n",
        "\n",
        "    This function takes an array of softmax output probabilities\n",
        "    (usually from a neural network's output layer) and converts\n",
        "    each row to a one-hot encoded vector. The highest probability\n",
        "    in each row is marked as 1, with all other values set to 0.\n",
        "\n",
        "    Parameters:\n",
        "    input (numpy.ndarray): A 2D array where each row contains softmax probabilities for each class.\n",
        "                            The shape of the array is (n_samples, n_classes).\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: A 2D array of the same shape as the input, where each row is the one-hot encoded representation\n",
        "                   of the class with the highest probability in the original row.\n",
        "    \"\"\"\n",
        "\n",
        "    output = np.zeros_like(input, dtype=int)\n",
        "    output[np.arange(len(input)), np.argmax(input, axis=1)] = 1\n",
        "    return output\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# CUSTOM CNN CLASS\n",
        "#-------------------------------------------------------------------------------\n",
        "class CNNClassifier:\n",
        "    \"\"\"\n",
        "    A Convolutional Neural Network (CNN) classifier using Keras, customized for binary classification tasks.\n",
        "\n",
        "    This class wraps a Keras Sequential model with a specific architecture suitable for image classification tasks.\n",
        "    It includes a custom `predict` method that outputs one-hot encoded predictions, and other standard Keras model\n",
        "    methods are accessible as well. This was done to override the need for the SciKeras wrappers that is frequently\n",
        "    incompatible with Google Colab versions of Keras & Tensorflow. Feel free to modify as needed.\n",
        "\n",
        "    Attributes:\n",
        "        num_epochs (int): The number of training epochs.\n",
        "        layers (int): The number of convolutional layers in the model.\n",
        "        dropout (float): The dropout rate used in dropout layers for regularization.\n",
        "        model (keras.models.Sequential): The underlying Keras Sequential model.\n",
        "\n",
        "    Methods:\n",
        "        build_model(): Constructs the CNN model with the specified architecture and compiles it.\n",
        "\n",
        "        fit(*args, **kwargs): Trains the model. Accepts arguments compatible with the Keras `fit` method.\n",
        "\n",
        "        predict(*args, **kwargs): Predicts labels for the input data. Converts the softmax output of the model\n",
        "                                  to one-hot encoded format using `logits_to_one_hot_encoding`. Necessary to match\n",
        "                                  accuracy_score function expected arguments.\n",
        "\n",
        "        predict_proba(*args, **kwargs): Predicts labels for the input data and returns the raw output of the softmax.\n",
        "                                        Used when wanting to inspect the raw probabilistic scoring of the model.\n",
        "\n",
        "    Usage:\n",
        "        cnn_classifier = CNNClassifier(num_epochs=30, layers=4, dropout=0.5)\n",
        "        cnn_classifier.fit(X_train, y_train)\n",
        "        predictions = cnn_classifier.predict(X_test)\n",
        "\n",
        "    Note:\n",
        "        The `__getattr__` method is overridden to delegate attribute access to the underlying Keras model,\n",
        "        except for the `predict` method which is customized.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_epochs=30, layers=4, dropout=0.5):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.layers = layers\n",
        "        self.dropout = dropout\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Reshape((32, 32, 3)))\n",
        "\n",
        "        for i in range(self.layers):\n",
        "          model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "          model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Conv2D(32, (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(self.dropout))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Conv2D(64, (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(self.dropout))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(self.dropout))\n",
        "        model.add(Dense(2))\n",
        "        model.add(Activation('softmax'))\n",
        "        opt = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        return self.model.fit(*args, epochs=self.num_epochs, batch_size=10, verbose=2, **kwargs)\n",
        "\n",
        "    # NOTE: WRITTEN TO RETURN ONE HOT ENCODINGS FOR ACCURACY\n",
        "    def predict(self, *args, **kwargs):\n",
        "        predictions = self.model.predict(*args, **kwargs)\n",
        "        return logits_to_one_hot_encoding(predictions)\n",
        "\n",
        "    def predict_proba(self, *args, **kwargs):\n",
        "        predictions = self.model.predict(*args, **kwargs)\n",
        "        return predictions\n",
        "\n",
        "    def score(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        if name != 'predict' and name != 'predict_proba':\n",
        "            return getattr(self.model, name)\n",
        "        else:\n",
        "            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n",
        "            # Load our data\n",
        "data_raw, labels_raw = load_data()\n",
        "data = data_raw.astype(float)\n",
        "labels = categorical_to_numpy(labels_raw)\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(data, labels, test_size=0.2, random_state=1)\n",
        "\n",
        "### WRITE YOUR CODE BELOW: Print the image data and its shape using the shape attribute!\n",
        "plot_one_image(data_raw, labels_raw, 300) # Play around with the number!\n",
        "# Create and train our CNN model CNNClassifier\n",
        "cnn = CNNClassifier(num_epochs=5, layers=2, dropout=0.5)\n",
        "cnn.fit(X_train, y_train)\n",
        "\n",
        "# Print the score on the testing data\n",
        "print(cnn.score(X_test, y_test))\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
        "plot_acc(history)\n",
        "\n",
        "# Print the score on the testing data\n",
        "print(\"CNN Testing Set Score:\")\n",
        "print(cnn.score(X_test, y_test))\n",
        "\n",
        "# This network can be described as:\n",
        "\n",
        "# Input Layer: 3 neurons\n",
        "# Layer 1 (Hidden): 4 neurons that are activated by 'relu'\n",
        "# Layer 2 (Output): 2 neurons that are activated by 'softmax'\n",
        "# We also want to compile the model with loss = 'categorical_crossentropy'\n",
        "## WRITE YOUR CODE BELOW: delete the #s and fill in the blanks!\n",
        "\n",
        "# model_1 = Sequential()\n",
        "# model_1.add(InputLayer(shape=(____,)))\n",
        "# model_1.add(Dense(____, activation='____'))\n",
        "# model_1.add(Dense(____, activation='____'))\n",
        "# model_1.compile(loss='____', optimizer='adam', metrics=['accuracy'])\n",
        "# model_1.predict(np.array([[14, 18, 5]])) # Try any input! This represents an animal of height 14, weight 18, and age 5\n",
        "\n",
        "# ReLU (Rectified Linear Unit)\n",
        "# Usage: ReLU is primarily used in the hidden layers of neural networks.\n",
        "# Function: It outputs the input directly if it is positive; otherwise, it will output zero. Mathematically, it's defined as\n",
        "\n",
        "# Advantages:\n",
        "# Helps in speeding up the training process by overcoming the vanishing gradient problem common with other activation functions like sigmoid or tanh.\n",
        "# It introduces non-linearity into the network, allowing it to learn more complex patterns.\n",
        "# Softmax\n",
        "# Usage: Softmax is typically used in the output layer of a classifier, where we need to handle multiple classes.\n",
        "# Function: Softmax converts logits (the raw output scores in logistic regression) into probabilities by taking the exponentials of each output and then normalizing these values by dividing by the sum of all exponentials. This gives a probability distribution across various classes that sums to 1. Mathematically, for each output\n",
        "#  in the layer, it's defined as\n",
        "\n",
        "\n",
        "# Advantages:\n",
        "# By outputting a probability distribution, it works well for classes that are mutually exclusive, making it ideal for multi-class classification problems.\n",
        "model = Sequential()\n",
        "model.add(Reshape((32, 32, 3))) # Try to figure out why this layer is necessary!\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "###\n",
        "### WRITE YOUR CODE BELOW: Add more layers!\n",
        "###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "### END CODE HERE\n",
        "###\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN and plot accuracy.\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=70)\n",
        "plot_acc(history)\n",
        "model.add(Conv2D(96, 11, strides=3))\n",
        "model.add(Activation('relu'))\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# 1. Number of Layers\n",
        "# Impact: More layers allow a CNN to learn more complex features at different levels of abstraction. For example, deeper layers in models like VGG16 are adept at recognizing high-level features in images.\n",
        "# Example: model.add(Conv2D(32, (3, 3))) followed by multiple similar layers.\n",
        "# 2. Number of Filters\n",
        "# Impact: Each filter detects different features, and having more filters increases the network's capacity to learn diverse features.\n",
        "# Example: Starting with model.add(Conv2D(32, (3, 3))) and increasing to model.add(Conv2D(64, (3, 3))) in subsequent layers.\n",
        "# 3. Filter Size\n",
        "# Impact: Smaller filters (e.g., 3x3) are excellent for capturing small detail, while larger filters (e.g., 5x5) capture wider patterns.\n",
        "# Example: model.add(Conv2D(64, (3, 3))) for fine detail versus model.add(Conv2D(64, (5, 5))) for broader features.\n",
        "# 4. Stride\n",
        "# Impact: Strides affect how the filter moves across the image; larger strides reduce the output size.\n",
        "# Example: model.add(Conv2D(64, (3, 3), strides=(2, 2))), reducing spatial dimensions more rapidly.\n",
        "# 5. Padding\n",
        "# Impact: 'Same' padding ensures the output has the same width and height as the input, while 'valid' does not add zero padding.\n",
        "# Example: model.add(Conv2D(64, (3, 3), padding='same')) keeps dimensions intact.\n",
        "# 6. Activation Function\n",
        "# Impact: Determines how neurons fire. Commonly used ReLU is effective for non-linear problems.\n",
        "# Example: model.add(Activation('relu')) typically follows each convolutional layer.\n",
        "# 7. Pooling Layer\n",
        "# Impact: Reduces dimensionality, thus controlling overfitting and reducing computational load.\n",
        "# Example: model.add(MaxPooling2D((2, 2))) often follows one or more convolutional layers.\n",
        "# 8. Learning Rate\n",
        "# Impact: A crucial factor in convergence speed. Too high can overshoot minimum; too low may result in a long training process.\n",
        "# Example: optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "# 9. Batch Size\n",
        "# Impact: Affects the memory footprint and can influence the model's generalization.\n",
        "# Example: model.fit(x_train, y_train, batch_size=64)\n",
        "# 10. Epochs\n",
        "# Impact: More epochs generally lead to a better fit, provided early stopping is used to prevent overfitting.\n",
        "# Example: model.fit(x_train, y_train, epochs=20)\n",
        "#@title Run this to load cat and dog data! {\"display-mode\":\"form\", \"form-width\":\"25%\"}\n",
        "\n",
        "# Code here from https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb#scrollTo=4PIP1rkmeAYS\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "  road_model = model\n",
        "  road_saved = True\n",
        "except NameError:\n",
        "  road_saved = False\n",
        "\n",
        "IMG_SHAPE  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filtered.zip', origin=_URL, extract=True)\n",
        "base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered_extracted/cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
        "train_image_generator      = ImageDataGenerator()  # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator()  # Generator for our validation data\n",
        "train_data = next(train_image_generator.flow_from_directory(batch_size=2000,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
        "                                                           class_mode='binary'))\n",
        "val_data = next(validation_image_generator.flow_from_directory(batch_size=1000,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              shuffle=False,\n",
        "                                                              target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
        "\n",
        "                                                              class_mode='binary'))\n",
        "cd_train_inputs, cd_train_labels = train_data\n",
        "cd_test_inputs, cd_test_labels = val_data\n",
        "\n",
        "index = np.random.randint(len(cd_train_inputs))\n",
        "plt.imshow(cd_train_inputs[index] / 255)\n",
        "plt.show()\n",
        "print(\"Label:\", cd_train_labels[index])\n",
        "\n",
        "#From AlexNet to VGGNet: A Better Choice for Image Classification\n",
        "print(cd_train_inputs.shape)\n",
        "print(cd_train_labels.shape)\n",
        "print(cd_test_inputs.shape)\n",
        "print(cd_test_labels.shape)\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "Below, we explore these parameters through practical examples directly applied in Keras.\n",
        "\n",
        "train_data = next(train_image_generator.flow_from_directory(batch_size=2000,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(224,224), #(150,150)\n",
        "                                                           class_mode='binary'))\n",
        "val_data = next(validation_image_generator.flow_from_directory(batch_size=1000,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              shuffle=False,\n",
        "                                                              target_size=(224,224), #(150,150)\n",
        "                                                              class_mode='binary'))\n",
        "cd_train_inputs, cd_train_labels = train_data\n",
        "cd_test_inputs, cd_test_labels = val_data\n",
        "\n",
        "###\n",
        "### WRITE YOUR CODE BELOW: Fill in the blanks!\n",
        "###\n",
        "\n",
        "# Load the VGG16 model without the top (classifier) layers\n",
        "base_model = VGG16(include_top=False, input_shape=(______,______,______))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classifier on top\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(128, activation='______', kernel_initializer='he_uniform')(x)\n",
        "output = Dense(2, activation='______')(x)\n",
        "\n",
        "###\n",
        "### END CODE HERE\n",
        "###\n",
        "\n",
        "\n",
        "# Define new model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    cd_train_inputs, to_categorical(cd_train_labels),\n",
        "    validation_data=(cd_test_inputs, to_categorical(cd_test_labels)),\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "# Display training history and model structure\n",
        "plot_acc(history)\n",
        "model.summary()"
      ]
    }
  ]
}